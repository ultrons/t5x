# Defaults for pretraining with train.py.
#
# See go/t5x-pretrain for instructions.
#
# You must also include a binding for MODEL.
#
# Required to be set:
#
# - MIXTURE_OR_TASK_NAME
# - TASK_FEATURE_LENGTHS
# - TRAIN_STEPS
# - MODEL_DIR: # automatically set when using xm_launch
#
# Commonly overridden options:
#
# - train/DatasetConfig.batch_size
# - train_eval/DatasetConfig.batch_size
# - PjitPartitioner.num_partitions
# - Trainer.num_microbatches
# - DROPOUT_RATE
from __gin__ import dynamic_registration

import __main__ as train_script
from t5x import gin_utils
from t5x import partitioning
from t5x import utils
from t5x import trainer
from t5x import adafactor

MIXTURE_OR_TASK_NAME = %gin.REQUIRED
TASK_FEATURE_LENGTHS = %gin.REQUIRED
TRAIN_STEPS = %gin.REQUIRED
MODEL_DIR = %gin.REQUIRED
USE_CACHED_TASKS = False

# DEPRECATED: Import the this module in your gin file.
MIXTURE_OR_TASK_MODULE = None
SHUFFLE_TRAIN_EXAMPLES = True

# Can be overridden with `train.*`.`
train_script.train:
  model = %MODEL  # imported from separate gin file
  model_dir = %MODEL_DIR
  train_dataset_cfg = @train/utils.DatasetConfig()
  train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
  infer_eval_dataset_cfg = None
  checkpoint_cfg = @utils.CheckpointConfig()
  #partitioner = @partitioning.PjitPartitioner()
  partitioner = @partitioning.ModelBasedPjitPartitioner()
  trainer_cls = @trainer.Trainer
  total_steps = %TRAIN_STEPS
  eval_steps = 20
  eval_period = 1000
  random_seed = 42  # None to use faster, hardware RNG?
  summarize_config_fn = @gin_utils.summarize_gin_config

train/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = 4
  shuffle = %SHUFFLE_TRAIN_EXAMPLES
  seed = 42  # If None it will use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

train_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = 128
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

utils.CheckpointConfig:
  restore = None  # initialize from scratch
  save = @utils.SaveCheckpointConfig()
utils.SaveCheckpointConfig:
  period = 1000
  dtype = 'float32'
  keep = None  # keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state

#partitioning.PjitPartitioner:
#  num_partitions = 8
#  model_parallel_submesh = ()
#  parameter_partitioning_dims = 1

partitioning.ModelBasedPjitPartitioner:
  num_partitions = 128
  model_parallel_submesh = (2,8,8,1)

trainer.Trainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()
utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 0.001
  warmup_steps = 1000
adafactor.Adafactor:
  logical_factor_rules = @adafactor.standard_logical_factor_rules()
